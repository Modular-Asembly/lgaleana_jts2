{
  "name": "jts2",
  "user": "lgaleana",
  "architecture": [
    {
      "design": {
        "type": "infrastructure",
        "name": "CloudRun",
        "namespace": "External",
        "config": {}
      },
      "files": [],
      "update_status": "up_to_date",
      "is_deployed": false
    },
    {
      "design": {
        "type": "logic",
        "name": "main",
        "namespace": "app",
        "dependencies": [
          "app.core.database.sql_adaptor",
          "app.pipeline.pipeline_endpoint"
        ],
        "purpose": "1) Calls load_dotenv() before any other import.\n2) Initializes the FastAPI app.\n3) Adds CORSMiddleware with *.\n4) Adds all the application routers.\n5) Calls Base.metadata.create_all(engine).",
        "pypi_packages": [
          "fastapi==0.115.6",
          "pydantic==2.10.4",
          "python-dotenv==1.0.1",
          "uvicorn==0.34.0"
        ],
        "is_endpoint": false
      },
      "files": [
        {
          "path": "app/main.py",
          "content": "import os\nfrom dotenv import load_dotenv\n\n# Load environment variables before any other import\nload_dotenv()\n\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom app.core.database.sql_adaptor import Base, engine\nfrom app.pipeline.pipeline_endpoint import router as pipeline_router\nfrom typing import Any\n\ndef create_app() -> FastAPI:\n    \"\"\"\n    Creates and configures the FastAPI application.\n\n    Steps:\n      1. Initializes the FastAPI app.\n      2. Configures CORS middleware to allow all origins, credentials, methods, and headers.\n      3. Includes the pipeline router.\n      4. Calls Base.metadata.create_all(engine) to create all database tables.\n\n    Returns:\n        A configured FastAPI application instance.\n    \"\"\"\n    app: FastAPI = FastAPI()\n\n    # Configure CORS middleware with all origins allowed.\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"]\n    )\n\n    # Include the pipeline router.\n    app.include_router(pipeline_router)\n\n    # Create all tables defined in the SQLAlchemy models.\n    Base.metadata.create_all(bind=engine)\n\n    return app\n\n# Create the FastAPI application instance to be imported elsewhere.\napp: FastAPI = create_app()\n"
        }
      ],
      "update_status": "up_to_date",
      "is_deployed": false
    },
    {
      "design": {
        "type": "infrastructure",
        "name": "CloudSQL",
        "namespace": "External",
        "config": {}
      },
      "files": [],
      "update_status": "up_to_date",
      "is_deployed": false
    },
    {
      "design": {
        "type": "logic",
        "name": "sql_adaptor",
        "namespace": "app.core.database",
        "dependencies": [],
        "purpose": "1) Initializes Base, engine and SessionLocal. Uses the DB_URL environment variable.\n2) Defines get_session to yield a session.",
        "pypi_packages": [
          "psycopg2-binary==2.9.10",
          "sqlalchemy==2.0.36"
        ],
        "is_endpoint": false
      },
      "files": [
        {
          "path": "app/core/database/sql_adaptor.py",
          "content": "import os\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker, declarative_base, Session\nfrom typing import Iterator\n\n# Retrieve the database URL from environment variables.\nDB_URL: str = os.getenv(\"DB_URL\", \"\")\nif not DB_URL:\n    raise EnvironmentError(\"DB_URL environment variable is not set.\")\n\n# Initialize the SQLAlchemy engine using the provided DB_URL.\nengine = create_engine(DB_URL, echo=False)\n\n# Create a configured \"SessionLocal\" class.\nSessionLocal = sessionmaker(bind=engine, autocommit=False, autoflush=False)\n\n# Create a Base class for declarative class definitions.\nBase = declarative_base()\n\ndef get_session() -> Iterator[Session]:\n    \"\"\"\n    Creates a new SQLAlchemy Session, yields it for use in database interactions,\n    and ensures it is closed after its usage.\n    \"\"\"\n    session: Session = SessionLocal()\n    try:\n        yield session\n    finally:\n        session.close()\n"
        }
      ],
      "update_status": "up_to_date",
      "is_deployed": false
    },
    {
      "design": {
        "type": "logic",
        "name": "salesforce_query",
        "namespace": "app.pipeline",
        "dependencies": [],
        "purpose": "Queries data from Salesforce using its API. The module performs the following steps:\n1. Obtains an OAuth 2.0 access token by making a POST request to the hardcoded Salesforce OAuth token endpoint (https://login.salesforce.com/services/oauth2/token).\n2. Uses the environment variables SALESFORCE_CLIENT_ID, SALESFORCE_CLIENT_SECRET, SALESFORCE_USERNAME, SALESFORCE_PASSWORD to authenticate. The password is combined with the security token if provided.\n3. Initializes a Salesforce connection using the obtained access token and instance_url, with the Salesforce API version hardcoded to \"55.0\".\n4. Executes a SOQL query with the following hardcoded details:\n   - Query texts:\n     \u2022 SELECT Id, GCLID__c, Name, Original_Lead_Created_Date_Time__c, Admission_Date__c \n     \u2022 FROM Opportunity \n     \u2022 WHERE StageName IN ('Admitted', 'Alumni') \n     \u2022 AND Original_Lead_Created_Date_Time__c = LAST_90_DAYS\n5. After querying, the module removes any extraneous 'attributes' field added by simple_salesforce from each record.\n6. Additionally, it filters out any records where the GCLID__c field is null, ensuring that only records with a valid GCLID value are returned.",
        "pypi_packages": [
          "simple-salesforce==1.12.4"
        ],
        "is_endpoint": false
      },
      "files": [
        {
          "path": "app/pipeline/salesforce_query.py",
          "content": "import os\nimport requests\nfrom typing import List, Dict, Any\nfrom simple_salesforce import Salesforce\n\ndef get_salesforce_access_token() -> Dict[str, str]:\n    \"\"\"\n    Obtains an OAuth 2.0 access token from Salesforce.\n\n    This function makes a POST request to the hardcoded Salesforce OAuth token endpoint using\n    the following environment variables:\n      - SALESFORCE_CLIENT_ID\n      - SALESFORCE_CLIENT_SECRET\n      - SALESFORCE_USERNAME\n      - SALESFORCE_PASSWORD\n      - SALESFORCE_SECURITY_TOKEN (optional)\n\n    The password is combined with the security token (if provided) to authenticate.\n    \n    Returns:\n        A dictionary containing 'access_token' and 'instance_url'.\n    \n    Raises:\n        EnvironmentError: If any of the required environment variables (except the security token) is missing.\n        requests.HTTPError: If the POST request fails.\n        ValueError: If the response JSON does not contain the required keys.\n    \"\"\"\n    token_url = \"https://login.salesforce.com/services/oauth2/token\"\n    client_id = os.getenv(\"SALESFORCE_CLIENT_ID\")\n    client_secret = os.getenv(\"SALESFORCE_CLIENT_SECRET\")\n    username = os.getenv(\"SALESFORCE_USERNAME\")\n    password = os.getenv(\"SALESFORCE_PASSWORD\")\n    security_token = os.getenv(\"SALESFORCE_SECURITY_TOKEN\", \"\")\n\n    if not (client_id and client_secret and username and password):\n        raise EnvironmentError(\"One or more required Salesforce OAuth environment variables are not set.\")\n\n    combined_password = password + security_token\n\n    payload = {\n        \"grant_type\": \"password\",\n        \"client_id\": client_id,\n        \"client_secret\": client_secret,\n        \"username\": username,\n        \"password\": combined_password\n    }\n\n    response = requests.post(token_url, data=payload)\n    response.raise_for_status()\n    auth_response = response.json()\n\n    if \"access_token\" not in auth_response or \"instance_url\" not in auth_response:\n        raise ValueError(\"Salesforce OAuth response missing access_token or instance_url.\")\n\n    return {\n        \"access_token\": auth_response[\"access_token\"],\n        \"instance_url\": auth_response[\"instance_url\"]\n    }\n\ndef get_salesforce_connection() -> Salesforce:\n    \"\"\"\n    Initializes and returns a Salesforce connection using the simple_salesforce library.\n\n    Uses the OAuth access token and instance_url obtained from get_salesforce_access_token.\n    The Salesforce API version is hardcoded to \"55.0\".\n    \n    Returns:\n        An instance of simple_salesforce.Salesforce.\n    \n    Raises:\n        Any exceptions raised from get_salesforce_access_token or the connection initialization.\n    \"\"\"\n    auth_data = get_salesforce_access_token()\n    sf = Salesforce(\n        instance_url=auth_data[\"instance_url\"],\n        session_id=auth_data[\"access_token\"],\n        version=\"55.0\"\n    )\n    return sf\n\ndef query_salesforce() -> List[Dict[str, Any]]:\n    \"\"\"\n    Queries Salesforce data using its API.\n\n    Executes the following SOQL query with hardcoded parameters:\n      \u2022 SELECT Id, GCLID__c, Name, Original_Lead_Created_Date_Time__c, Admission_Date__c\n      \u2022 FROM Opportunity\n      \u2022 WHERE StageName IN ('Admitted', 'Alumni')\n      \u2022 AND Original_Lead_Created_Date_Time__c = LAST_90_DAYS\n\n    After retrieving the results, the function:\n      1. Removes the extraneous 'attributes' field from each record.\n      2. Filters out records where the 'GCLID__c' field is null.\n\n    Returns:\n        A list of dictionaries representing the cleaned Salesforce records.\n    \n    Raises:\n        Any exceptions raised during the query or data processing.\n    \"\"\"\n    sf = get_salesforce_connection()\n    soql_query = (\n        \"SELECT Id, GCLID__c, Name, Original_Lead_Created_Date_Time__c, Admission_Date__c \"\n        \"FROM Opportunity \"\n        \"WHERE StageName IN ('Admitted', 'Alumni') \"\n        \"AND Original_Lead_Created_Date_Time__c = LAST_90_DAYS\"\n    )\n    result = sf.query_all(soql_query)\n    records = result.get(\"records\", [])\n    \n    # Remove extraneous 'attributes' field from each record.\n    for record in records:\n        record.pop(\"attributes\", None)\n    \n    # Filter out records with a null GCLID__c field.\n    filtered_records = [record for record in records if record.get(\"GCLID__c\") is not None]\n    return filtered_records\n"
        }
      ],
      "update_status": "up_to_date",
      "is_deployed": false
    },
    {
      "design": {
        "type": "logic",
        "name": "filter_unprocessed",
        "namespace": "app.pipeline",
        "dependencies": [
          "app.core.database.sql_adaptor",
          "app.models.UploadStatus"
        ],
        "purpose": "Filters out rows that have already been processed. It takes the raw sales data provided as input (from salesforce_query when called by the orchestration endpoint) and returns only those rows that haven't been processed yet. This module may also log a count of filtered rows.",
        "pypi_packages": [],
        "is_endpoint": false
      },
      "files": [
        {
          "path": "app/pipeline/filter_unprocessed.py",
          "content": "import os\nfrom typing import List, Dict, Any\nfrom app.core.database.sql_adaptor import SessionLocal\nfrom app.models.upload_status import UploadStatus\n\ndef filter_unprocessed(sales_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"\n    Filters out rows that have already been processed.\n    It checks each record from the raw sales data (provided by salesforce_query)\n    against the UploadStatus records in the database.\n    \n    A record is considered processed if its Salesforce ID is already present in the database.\n    \n    Args:\n        sales_data: List of dictionaries containing sales data, where each dictionary\n                    is expected to have an \"Id\" key representing the Salesforce Opportunity Id.\n    \n    Returns:\n        List of dictionaries that haven't been processed yet.\n        \n    Raises:\n        Any exceptions raised by database queries will propagate.\n    \"\"\"\n    # Open a new database session.\n    with SessionLocal() as session:\n        # Retrieve a set of all processed Salesforce IDs from the upload_status table.\n        processed_records = session.query(UploadStatus.salesforce_id).all()\n        processed_ids = {record[0] for record in processed_records}\n\n    # Filter sales data to include only rows which haven't been processed.\n    filtered_data = [row for row in sales_data if row.get(\"Id\") not in processed_ids]\n\n    # Log the count of filtered records.\n    filtered_count = len(sales_data) - len(filtered_data)\n    print(f\"[filter_unprocessed] Filtered out {filtered_count} records that were already processed.\")\n\n    return filtered_data\n"
        }
      ],
      "update_status": "up_to_date",
      "is_deployed": false
    },
    {
      "design": {
        "type": "logic",
        "name": "google_ads_upload",
        "namespace": "app.pipeline",
        "dependencies": [
          "app.core.google_auth.google_auth"
        ],
        "purpose": "Uploads conversions to Google Ads using the Google Ads API. This module takes filtered records (from filter_unprocessed) and performs conversion uploads through a REST API call. It requires an access token obtained from the google_auth module (app.core.google_auth.google_auth) for authentication. The module processes each filtered record by extracting the sales lead creation time from either 'Original_Lead_Created_Date_Time__c' or 'Admission_Date__c' and the GCLID from 'GCLID__c'.\n\nHardcoded values and details:\n\u2022 action_id: The conversion action identifier is hardcoded to 462477827.\n\u2022 API version: The Google Ads API version is hardcoded to \"v18\".\n\u2022 Date formatting: The input datetime is expected in ISO format with timezone information (formatted as \"%Y-%m-%dT%H:%M:%S.%f%z\"). It is then reformatted to \"%Y-%m-%d %H:%M:%S%z\" with an adjustment to the timezone format by inserting a colon.\n\u2022 Conversion details: Each conversion object is created with a static conversion value of 1 and a currency code of \"USD\".\n\u2022 Endpoint URL: The endpoint URL is built as \"https://googleads.googleapis.com/{api_version}/customers/{customer_id}:uploadClickConversions\" where api_version and customer_id are injected from the hardcoded API version ('v18') and environment variable GADS_CUSTOMER respectively.\n\u2022 Request headers: In addition to the Authorization header (Bearer token), headers include \"developer-token\" (from environment variable GADS_DEVELOPER_TOKEN) and \"login-customer-id\" (from environment variable GADS_LOGIN_CUSTOMER_ID).\n\u2022 Partial failure handling: If the response contains a 'partialFailureError', it extracts error details by mapping error indices to the individual conversion objects.\n\nThis module returns a list of result objects that include either error details or the result of a successful conversion upload.",
        "pypi_packages": [
          "google-ads==22.0.0"
        ],
        "is_endpoint": false
      },
      "files": [
        {
          "path": "app/pipeline/google_ads_upload.py",
          "content": "import os\nimport requests\nfrom datetime import datetime\nfrom typing import List, Dict, Any\n\nfrom app.core.google_auth.google_auth import get_access_token\n\n\ndef upload_conversions(filtered_records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    action_id = 462477827\n    customer_id = os.getenv(\"GADS_CUSTOMER\")\n    api_version = \"v18\"\n\n    access_token = get_access_token()\n\n    conversion_objects = []\n    for record in filtered_records:\n        lead_created_time = record.get(\n            \"Original_Lead_Created_Date_Time__c\"\n        ) or record.get(\"Admission_Date__c\")\n        gclid = record.get(\"GCLID__c\")\n\n        if not (gclid and lead_created_time):\n            continue\n\n        formatted_date = datetime.strptime(lead_created_time, \"%Y-%m-%dT%H:%M:%S.%f%z\")\n        formatted_date = formatted_date.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n        formatted_date = formatted_date[:-2] + \":\" + formatted_date[-2:]\n\n        conversion = {\n            \"conversionAction\": f\"customers/{customer_id}/conversionActions/{action_id}\",\n            \"gclid\": gclid,\n            \"conversionValue\": 1,\n            \"conversionDateTime\": formatted_date,\n            \"currencyCode\": \"USD\",\n        }\n        conversion_objects.append(conversion)\n\n    if not conversion_objects:\n        return []\n\n    url = f\"https://googleads.googleapis.com/{api_version}/customers/{customer_id}:uploadClickConversions\"\n    headers = {\n        \"Authorization\": f\"Bearer {access_token}\",\n        \"Content-Type\": \"application/json\",\n        \"developer-token\": os.getenv(\"GADS_DEVELOPER_TOKEN\"),\n        \"login-customer-id\": os.getenv(\"GADS_LOGIN_CUSTOMER_ID\"),\n    }\n    payload = {\n        \"conversions\": conversion_objects,\n        \"partialFailure\": True,\n    }\n\n    response = requests.post(url, headers=headers, json=payload)\n    if not response.ok:\n        print(response.text)\n        response.raise_for_status()\n\n    conversion_response = response.json()\n    results = []\n    if \"partialFailureError\" in conversion_response:\n        # Extract error details\n        error_details = conversion_response[\"partialFailureError\"][\"details\"][0][\n            \"errors\"\n        ]\n        error_by_index = {\n            error[\"location\"][\"fieldPathElements\"][0][\"index\"]: error\n            for error in error_details\n        }\n\n        # Map results with original GCLIDs\n        for idx, original_conversion in enumerate(conversion_objects):\n            result = {\"gclid\": original_conversion[\"gclid\"]}\n            if idx in error_by_index:\n                result[\"error\"] = error_by_index[idx]\n            else:\n                # Copy successful conversion data\n                result.update(conversion_response[\"results\"][idx])\n            results.append(result)\n    else:\n        results = conversion_response.get(\"results\", [])\n\n    return results\n"
        }
      ],
      "update_status": "up_to_date",
      "is_deployed": false
    },
    {
      "design": {
        "type": "logic",
        "name": "store_success",
        "namespace": "app.pipeline",
        "dependencies": [
          "app.core.database.sql_adaptor",
          "app.models.UploadStatus"
        ],
        "purpose": "Stores details of successful uploads into a database. This module logs each successful conversion upload along with metadata (e.g., timestamp, conversion id, status) for further audit and reporting. It utilizes SQLAlchemy sessions from the SQL adaptor for database interactions. The input data is provided from the google_ads_upload module when orchestrated by an endpoint.",
        "pypi_packages": [
          "sqlalchemy==2.0.36"
        ],
        "is_endpoint": false
      },
      "files": [
        {
          "path": "app/pipeline/store_success.py",
          "content": "import datetime\nfrom typing import List, Dict, Any\n\nfrom sqlalchemy.orm import Session\nfrom app.core.database.sql_adaptor import SessionLocal\nfrom app.models.upload_status import UploadStatus\n\ndef store_success_records(success_data: List[Dict[str, Any]]) -> List[UploadStatus]:\n    \"\"\"\n    Stores details of successful conversion uploads into the database.\n    \n    Each record in success_data is expected to be a dictionary with the following keys:\n      - 'salesforce_id': str - The Salesforce Opportunity Id.\n      - 'gclid': Optional[str] - The GCLID value from the Salesforce record.\n      - 'original_lead_created_datetime': datetime or ISO formatted str - When the lead was created.\n      - 'admission_date': datetime or ISO formatted str - The date of admission.\n      - 'status': str - The upload status, expected to be 'successful'.\n      - 'error_details': Optional[str] - Should be None or empty for successful uploads.\n      \n    Returns:\n        A list of UploadStatus instances that have been stored in the database.\n    \n    Raises:\n        Any exceptions raised during database operations.\n    \"\"\"\n    stored_records: List[UploadStatus] = []\n    \n    # Open a new database session.\n    with SessionLocal() as session:\n        # Iterate over each record from the input data.\n        for record in success_data:\n            # Parse datetime fields if they are provided as strings.\n            # If they are already datetime objects, they remain unchanged.\n            original_lead_created_datetime = record.get(\"original_lead_created_datetime\")\n            if isinstance(original_lead_created_datetime, str):\n                original_lead_created_datetime = datetime.datetime.fromisoformat(original_lead_created_datetime)\n            \n            admission_date = record.get(\"admission_date\")\n            if isinstance(admission_date, str):\n                admission_date = datetime.datetime.fromisoformat(admission_date)\n            \n            upload_status = UploadStatus(\n                salesforce_id=record[\"salesforce_id\"],\n                gclid=record.get(\"gclid\"),\n                original_lead_created_datetime=original_lead_created_datetime,\n                admission_date=admission_date,\n                status=record[\"status\"],\n                error_details=record.get(\"error_details\")\n            )\n            \n            session.add(upload_status)\n            stored_records.append(upload_status)\n        \n        session.commit()\n        # Refresh all instances to reflect data from the database (e.g., autogenerated id, timestamp)\n        for rec in stored_records:\n            session.refresh(rec)\n    \n    return stored_records\n"
        }
      ],
      "update_status": "up_to_date",
      "is_deployed": false
    },
    {
      "design": {
        "type": "logic",
        "name": "pipeline_endpoint",
        "namespace": "app.pipeline",
        "dependencies": [
          "app.pipeline.filter_unprocessed",
          "app.pipeline.google_ads_upload",
          "app.pipeline.salesforce_query",
          "app.pipeline.store_success"
        ],
        "purpose": "Defines a FastAPI endpoint that orchestrates the entire data pipeline by sequentially invoking several modules and aggregating their outputs. When this endpoint is accessed, it performs the following hardcoded steps:\n\n1. Calls salesforce_query to fetch data from Salesforce.\n2. Calls filter_unprocessed to filter out already processed records.\n3. Calls google_ads_upload to upload conversions to Google Ads.\n4. Uses a partitioning function to separate successful and failed upload results. The partitioning iterates over the upload results, matching each result with its corresponding record by GCLID. Success is determined by the presence of a success indicator in the result.\n5. Calls store_success to save successful conversions in the database.\n\nLogging is added between each step for debugging. Returns a list of the successful uploads.",
        "pypi_packages": [
          "fastapi==0.115.6"
        ],
        "is_endpoint": true
      },
      "files": [
        {
          "path": "app/pipeline/pipeline_endpoint.py",
          "content": "from fastapi import APIRouter\nfrom typing import List, Dict, Any, Tuple\nimport logging\n\nfrom app.pipeline.salesforce_query import query_salesforce\nfrom app.pipeline.filter_unprocessed import filter_unprocessed\nfrom app.pipeline.google_ads_upload import upload_conversions\nfrom app.pipeline.store_success import store_success_records\n\nrouter = APIRouter()\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\n\ndef partition_upload_results(\n    upload_results: List[Dict[str, Any]]\n) -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n    \"\"\"\n    Partitions the upload results into successful and failed conversions.\n\n    Success is determined by the absence of an \"error\" key in the result.\n    \n    Args:\n        upload_results: List of dictionaries returned by the google_ads_upload module.\n    \n    Returns:\n        A tuple containing two lists:\n         - successful_results: List of dictionaries for successful uploads.\n         - failed_results: List of dictionaries for failed uploads.\n    \"\"\"\n    successful_results = []\n    failed_results = []\n    for result in upload_results:\n        if \"error\" in result:\n            failed_results.append(result)\n        else:\n            successful_results.append(result)\n    return successful_results, failed_results\n\n\ndef map_success_to_store_data(\n    successful_results: List[Dict[str, Any]], filtered_records: List[Dict[str, Any]]\n) -> List[Dict[str, Any]]:\n    \"\"\"\n    Matches each successful upload result with its corresponding original sales record\n    (based on matching GCLID value) and prepares the data for storing success in the database.\n\n    The resulting dictionary includes:\n      - salesforce_id: The Opportunity Id from the original record.\n      - gclid: The GCLID value.\n      - original_lead_created_datetime: Value from \"Original_Lead_Created_Date_Time__c\" in the original record.\n      - admission_date: Value from \"Admission_Date__c\" in the original record.\n      - status: Static string \"successful\".\n      - error_details: None.\n\n    Args:\n        successful_results: The list of successful upload results from google_ads_upload.\n        filtered_records: The list of filtered sales records from Salesforce.\n\n    Returns:\n        List of dictionaries ready to be stored using store_success_records.\n    \"\"\"\n    # Create mapping from gclid to original record\n    gclid_to_record = {record.get(\"GCLID__c\"): record for record in filtered_records if record.get(\"GCLID__c\")}\n    store_data = []\n    for result in successful_results:\n        gclid = result.get(\"gclid\")\n        original_record = gclid_to_record.get(gclid)\n        if original_record:\n            data = {\n                \"salesforce_id\": original_record.get(\"Id\"),\n                \"gclid\": original_record.get(\"GCLID__c\"),\n                \"original_lead_created_datetime\": original_record.get(\"Original_Lead_Created_Date_Time__c\"),\n                \"admission_date\": original_record.get(\"Admission_Date__c\"),\n                \"status\": \"successful\",\n                \"error_details\": None,\n            }\n            store_data.append(data)\n    return store_data\n\n\n@router.get(\"/pipeline\", response_model=List[Dict[str, Any]])\nasync def orchestrate_pipeline() -> List[Dict[str, Any]]:\n    \"\"\"\n    Orchestrates the data pipeline by sequentially invoking:\n      1. salesforce_query to fetch Salesforce data.\n      2. filter_unprocessed to filter out already processed records.\n      3. google_ads_upload to upload conversions to Google Ads.\n      4. Partition the upload results into successful and failed conversions.\n      5. store_success to store successful conversion uploads into the database.\n    \n    Logging is added between each step for debugging.\n    \n    Returns:\n        A list of dictionaries representing the stored successful upload records.\n    \n    Raises:\n        Any exceptions from the modules invoked will propagate.\n    \"\"\"\n    logger.info(\"Starting pipeline orchestration.\")\n\n    # Step 1: Query Salesforce data.\n    sales_data: List[Dict[str, Any]] = query_salesforce()\n    logger.info(f\"Fetched {len(sales_data)} records from Salesforce.\")\n\n    # Step 2: Filter out already processed records.\n    filtered_data: List[Dict[str, Any]] = filter_unprocessed(sales_data)\n    logger.info(f\"{len(sales_data) - len(filtered_data)} records filtered out; {len(filtered_data)} records remain for processing.\")\n\n    # Step 3: Upload conversions to Google Ads.\n    upload_results: List[Dict[str, Any]] = upload_conversions(filtered_data)\n    logger.info(f\"Google Ads upload attempted on {len(filtered_data)} records; received {len(upload_results)} upload result(s).\")\n\n    # Step 4: Partition upload results into successful and failed.\n    successful_results, failed_results = partition_upload_results(upload_results)\n    logger.info(f\"{len(successful_results)} successful uploads; {len(failed_results)} failed uploads.\")\n\n    # Step 5: Prepare and store successful conversion data.\n    if successful_results:\n        store_data: List[Dict[str, Any]] = map_success_to_store_data(successful_results, filtered_data)\n        stored_successes = store_success_records(store_data)\n        logger.info(f\"Stored {len(stored_successes)} successful upload record(s) in the database.\")\n    else:\n        stored_successes = []\n        logger.info(\"No successful uploads to store.\")\n\n    return [record.__dict__ for record in stored_successes]\n"
        }
      ],
      "update_status": "up_to_date",
      "is_deployed": false
    },
    {
      "design": {
        "type": "datamodel",
        "name": "UploadStatus",
        "namespace": "app.models",
        "dependencies": [
          "app.core.database.sql_adaptor"
        ],
        "fields": [
          {
            "name": "id",
            "purpose": "Primary key for the upload record entry."
          },
          {
            "name": "salesforce_id",
            "purpose": "The Salesforce Opportunity Id from the query result."
          },
          {
            "name": "gclid",
            "purpose": "The GCLID value (GCLID__c) from the Salesforce record, used for conversion tracking."
          },
          {
            "name": "original_lead_created_datetime",
            "purpose": "The Original_Lead_Created_Date_Time__c field from the Salesforce record capturing when the lead was created."
          },
          {
            "name": "admission_date",
            "purpose": "The Admission_Date__c field from the Salesforce record indicating the date of admission."
          },
          {
            "name": "status",
            "purpose": "Upload status indicator, e.g., 'successful' or 'failed'."
          },
          {
            "name": "timestamp",
            "purpose": "The date and time when this upload status was recorded."
          },
          {
            "name": "error_details",
            "purpose": "Contains error details if the upload failed, otherwise null."
          }
        ]
      },
      "files": [
        {
          "path": "app/models/upload_status.py",
          "content": "from sqlalchemy import Column, Integer, String, DateTime, Text\nfrom app.core.database.sql_adaptor import Base\nimport datetime\n\nclass UploadStatus(Base):\n    __tablename__ = \"upload_status\"\n\n    id = Column(Integer, primary_key=True, index=True)\n    salesforce_id = Column(String, nullable=False, comment=\"Primary key for the upload record entry: Salesforce Opportunity Id from the query result.\")\n    gclid = Column(String, nullable=True, comment=\"The GCLID value (GCLID__c) from the Salesforce record, used for conversion tracking.\")\n    original_lead_created_datetime = Column(DateTime, nullable=False, comment=\"The Original_Lead_Created_Date_Time__c field from the Salesforce record capturing when the lead was created.\")\n    admission_date = Column(DateTime, nullable=False, comment=\"The Admission_Date__c field from the Salesforce record indicating the date of admission.\")\n    status = Column(String, nullable=False, comment=\"Upload status indicator, e.g., 'successful' or 'failed'.\")\n    timestamp = Column(DateTime, default=datetime.datetime.utcnow, nullable=False, comment=\"The date and time when this upload status was recorded.\")\n    error_details = Column(Text, nullable=True, comment=\"Contains error details if the upload failed, otherwise null.\")\n    \n    def __repr__(self):\n        return (f\"<UploadStatus(id={self.id}, salesforce_id='{self.salesforce_id}', \"\n                f\"gclid='{self.gclid}', original_lead_created_datetime='{self.original_lead_created_datetime}', \"\n                f\"admission_date='{self.admission_date}', status='{self.status}', \"\n                f\"timestamp='{self.timestamp}', error_details='{self.error_details}')>\")\n"
        },
        {
          "path": "app/models/upload_status_schema.py",
          "content": "from datetime import datetime\nfrom typing import Optional\nfrom pydantic import BaseModel\n\nclass UploadStatusBase(BaseModel):\n    salesforce_id: str\n    gclid: Optional[str] = None\n    original_lead_created_datetime: datetime\n    admission_date: datetime\n    status: str\n    error_details: Optional[str] = None\n\nclass UploadStatusCreate(UploadStatusBase):\n    pass\n\nclass UploadStatusRead(UploadStatusBase):\n    id: int\n    timestamp: datetime\n\n    class Config:\n        orm_mode = True\n"
        }
      ],
      "update_status": "up_to_date",
      "is_deployed": false
    },
    {
      "design": {
        "type": "logic",
        "name": "google_auth",
        "namespace": "app.core.google_auth",
        "dependencies": [],
        "purpose": "Handles Google OAuth authentication using credentials from environment variables. It uses OAUTH_CLIENT_ID, OAUTH_CLIENT_SECRET, and OAUTH_REFRESH_TOKEN to obtain and refresh access tokens. This module provides the access token required for interacting with the Google Ads API, Google Sheets API, and Gmail API.",
        "pypi_packages": [
          "google-auth==2.16.0"
        ],
        "is_endpoint": false
      },
      "files": [
        {
          "path": "app/core/google_auth/google_auth.py",
          "content": "import os\nfrom typing import Dict\nfrom google.oauth2.credentials import Credentials\nfrom google.auth.transport.requests import Request\n\n# OAuth token endpoint for Google\nGOOGLE_TOKEN_URI = \"https://oauth2.googleapis.com/token\"\n\ndef get_google_oauth_credentials() -> Credentials:\n    \"\"\"\n    Obtains Google OAuth credentials by using environment variables:\n      - OAUTH_CLIENT_ID\n      - OAUTH_CLIENT_SECRET\n      - OAUTH_REFRESH_TOKEN\n\n    Creates a Credentials object with no initial access token and refreshes it immediately,\n    returning valid credentials with an access token.\n    \n    Returns:\n        A google.oauth2.credentials.Credentials instance with a valid access token.\n    \n    Raises:\n        EnvironmentError: If one or more required environment variables are not set.\n        google.auth.exceptions.RefreshError: If the token refresh operation fails.\n    \"\"\"\n    client_id = os.getenv(\"OAUTH_CLIENT_ID\")\n    client_secret = os.getenv(\"OAUTH_CLIENT_SECRET\")\n    refresh_token = os.getenv(\"OAUTH_REFRESH_TOKEN\")\n\n    if not (client_id and client_secret and refresh_token):\n        raise EnvironmentError(\"One or more required environment variables (OAUTH_CLIENT_ID, OAUTH_CLIENT_SECRET, OAUTH_REFRESH_TOKEN) are not set.\")\n\n    # Construct a credentials object without an access token\n    creds = Credentials(\n        token=None,\n        refresh_token=refresh_token,\n        token_uri=GOOGLE_TOKEN_URI,\n        client_id=client_id,\n        client_secret=client_secret\n    )\n    \n    # Refresh the access token using a Request\n    creds.refresh(Request())\n    return creds\n\ndef get_access_token() -> str:\n    \"\"\"\n    Obtains a valid Google OAuth access token.\n    \n    Returns:\n        The access token as a string.\n    \n    Raises:\n        EnvironmentError: If required environment variables are missing.\n        google.auth.exceptions.RefreshError: If token refresh fails.\n    \"\"\"\n    creds = get_google_oauth_credentials()\n    return creds.token\n\ndef get_google_auth_headers() -> Dict[str, str]:\n    \"\"\"\n    Returns HTTP headers for authenticating against Google APIs.\n    The header contains the Bearer token.\n    \n    Returns:\n        A dictionary with the Authorization header.\n    \"\"\"\n    access_token = get_access_token()\n    return {\"Authorization\": f\"Bearer {access_token}\"}\n"
        }
      ],
      "update_status": "up_to_date",
      "is_deployed": false
    }
  ],
  "github": "https://github.com/Modular-Asembly/lgaleana_jts2"
}