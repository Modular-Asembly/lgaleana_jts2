{
  "name": "jts2",
  "user": "lgaleana",
  "architecture": [
    {
      "design": {
        "type": "infrastructure",
        "name": "CloudRun",
        "namespace": "External",
        "config": {}
      },
      "files": [],
      "update_status": "up_to_date",
      "is_deployed": false
    },
    {
      "design": {
        "type": "logic",
        "name": "main",
        "namespace": "app",
        "dependencies": [
          "app.core.database.sql_adaptor",
          "app.pipeline.pipeline_endpoint"
        ],
        "purpose": "1) Calls load_dotenv() before any other import.\n2) Initializes the FastAPI app.\n3) Adds CORSMiddleware with *.\n4) Adds all the application routers.\n5) Calls Base.metadata.create_all(engine).",
        "pypi_packages": [
          "fastapi==0.115.6",
          "pydantic==2.10.4",
          "python-dotenv==1.0.1",
          "uvicorn==0.34.0"
        ],
        "is_endpoint": false
      },
      "files": [
        {
          "path": "app/main.py",
          "content": "import os\nfrom dotenv import load_dotenv\n\n# Load environment variables before any other imports\nload_dotenv()\n\nfrom fastapi import FastAPI\nfrom fastapi.middleware.cors import CORSMiddleware\nfrom app.core.database.sql_adaptor import Base, engine\nfrom app.pipeline.pipeline_endpoint import router as pipeline_router\nfrom typing import Any\n\ndef create_app() -> FastAPI:\n    \"\"\"\n    Creates and configures the FastAPI application.\n    \n    Steps:\n      1. Initializes the FastAPI app.\n      2. Configures CORSMiddleware to allow all origins, credentials, methods, and headers.\n      3. Includes the application routers.\n      4. Calls Base.metadata.create_all(engine) to create all database tables.\n    \n    Returns:\n        A configured FastAPI application instance.\n    \"\"\"\n    app: FastAPI = FastAPI()\n\n    # Configure CORS middleware with all origins allowed\n    app.add_middleware(\n        CORSMiddleware,\n        allow_origins=[\"*\"],\n        allow_credentials=True,\n        allow_methods=[\"*\"],\n        allow_headers=[\"*\"]\n    )\n\n    # Include the pipeline router\n    app.include_router(pipeline_router)\n\n    # Create all tables defined in the SQLAlchemy models\n    Base.metadata.create_all(bind=engine)\n\n    return app\n\n# Create the FastAPI application instance to be imported elsewhere\napp: FastAPI = create_app()\n"
        }
      ],
      "update_status": "up_to_date",
      "is_deployed": false
    },
    {
      "design": {
        "type": "infrastructure",
        "name": "CloudSQL",
        "namespace": "External",
        "config": {}
      },
      "files": [],
      "update_status": "up_to_date",
      "is_deployed": false
    },
    {
      "design": {
        "type": "logic",
        "name": "sql_adaptor",
        "namespace": "app.core.database",
        "dependencies": [],
        "purpose": "1) Initializes Base, engine and SessionLocal. Uses the DB_URL environment variable.\n2) Defines get_session to yield a session.",
        "pypi_packages": [
          "psycopg2-binary==2.9.10",
          "sqlalchemy==2.0.36"
        ],
        "is_endpoint": false
      },
      "files": [
        {
          "path": "app/core/database/sql_adaptor.py",
          "content": "import os\nfrom sqlalchemy import create_engine\nfrom sqlalchemy.orm import sessionmaker, declarative_base, Session\nfrom typing import Iterator\n\n# Retrieve the database URL from environment variables.\nDB_URL: str = os.getenv(\"DB_URL\", \"\")\nif not DB_URL:\n    raise EnvironmentError(\"DB_URL environment variable is not set.\")\n\n# Initialize the SQLAlchemy engine using the provided DB_URL.\nengine = create_engine(DB_URL, echo=False)\n\n# Create a configured \"SessionLocal\" class.\nSessionLocal = sessionmaker(bind=engine, autocommit=False, autoflush=False)\n\n# Create a Base class for declarative class definitions.\nBase = declarative_base()\n\ndef get_session() -> Iterator[Session]:\n    \"\"\"\n    Creates a new SQLAlchemy Session, yields it for use in database interactions,\n    and ensures it is closed after its usage.\n    \"\"\"\n    session: Session = SessionLocal()\n    try:\n        yield session\n    finally:\n        session.close()\n"
        }
      ],
      "update_status": "up_to_date",
      "is_deployed": false
    },
    {
      "design": {
        "type": "logic",
        "name": "salesforce_query",
        "namespace": "app.pipeline",
        "dependencies": [],
        "purpose": "Queries data from Salesforce using its API. The module performs the following steps:\n1. Obtains an OAuth 2.0 access token by making a POST request to the hardcoded Salesforce OAuth token endpoint (https://login.salesforce.com/services/oauth2/token).\n2. Uses the environment variables SALESFORCE_CLIENT_ID, SALESFORCE_CLIENT_SECRET, SALESFORCE_USERNAME, SALESFORCE_PASSWORD to authenticate. The password is combined with the security token if provided.\n3. Initializes a Salesforce connection using the obtained access token and instance_url, with the Salesforce API version hardcoded to \"55.0\".\n4. Executes a SOQL query with the following hardcoded details:\n   - Query texts:\n     \u2022 SELECT Id, GCLID__c, Name, Original_Lead_Created_Date_Time__c, Admission_Date__c \n     \u2022 FROM Opportunity \n     \u2022 WHERE StageName IN ('Admitted', 'Alumni') \n     \u2022 AND Original_Lead_Created_Date_Time__c = LAST_90_DAYS\n5. After querying, the module removes any extraneous 'attributes' field added by simple_salesforce from each record.\n6. Additionally, it filters out any records where the GCLID__c field is null, ensuring that only records with a valid GCLID value are returned.",
        "pypi_packages": [
          "simple-salesforce==1.12.4"
        ],
        "is_endpoint": false
      },
      "files": [
        {
          "path": "app/pipeline/salesforce_query.py",
          "content": "import os\nimport requests\nfrom typing import List, Dict, Any\nfrom simple_salesforce import Salesforce\n\ndef get_salesforce_access_token() -> Dict[str, str]:\n    \"\"\"\n    Obtains an OAuth 2.0 access token from Salesforce using credentials from environment variables.\n    \n    Expected environment variables:\n    - SALESFORCE_CLIENT_ID\n    - SALESFORCE_CLIENT_SECRET\n    - SALESFORCE_USERNAME\n    - SALESFORCE_PASSWORD\n    - SALESFORCE_SECURITY_TOKEN (optional, if required by your Salesforce org)\n    \n    Returns:\n        A dictionary containing 'access_token' and 'instance_url'.\n    \"\"\"\n    token_url = \"https://login.salesforce.com/services/oauth2/token\"\n    client_id = os.getenv(\"SALESFORCE_CLIENT_ID\")\n    client_secret = os.getenv(\"SALESFORCE_CLIENT_SECRET\")\n    username = os.getenv(\"SALESFORCE_USERNAME\")\n    password = os.getenv(\"SALESFORCE_PASSWORD\")\n    # Optional security token\n    security_token = os.getenv(\"SALESFORCE_SECURITY_TOKEN\", \"\")\n\n    if not (client_id and client_secret and username and password):\n        raise EnvironmentError(\"One or more Salesforce OAuth environment variables are not set.\")\n\n    # Combine password and security token if token is provided\n    pwd_combined = password + security_token\n\n    payload = {\n        \"grant_type\": \"password\",\n        \"client_id\": client_id,\n        \"client_secret\": client_secret,\n        \"username\": username,\n        \"password\": pwd_combined\n    }\n\n    response = requests.post(token_url, data=payload)\n    response.raise_for_status()  # Let errors raise if the request failed\n    auth_response = response.json()\n\n    if \"access_token\" not in auth_response or \"instance_url\" not in auth_response:\n        raise ValueError(\"Salesforce OAuth response does not contain access_token or instance_url.\")\n\n    return {\n        \"access_token\": auth_response[\"access_token\"],\n        \"instance_url\": auth_response[\"instance_url\"]\n    }\n\ndef get_salesforce_connection() -> Salesforce:\n    \"\"\"\n    Creates and returns a Salesforce connection using an OAuth access token.\n    \n    Returns:\n        An instance of simple_salesforce.Salesforce.\n    \"\"\"\n    auth_data = get_salesforce_access_token()\n    # Create a Salesforce connection using the access token.\n    sf = Salesforce(instance_url=auth_data[\"instance_url\"], session_id=auth_data[\"access_token\"], version=\"55.0\")\n    return sf\n\ndef query_salesforce() -> List[Dict[str, Any]]:\n    \"\"\"\n    Queries Salesforce using its API. Executes the following SOQL query:\n    \n    SELECT Id, GCLID__c, Name, Original_Lead_Created_Date_Time__c, Admission_Date__c \n    FROM Opportunity \n    WHERE StageName IN ('Admitted', 'Alumni')\n    AND Original_Lead_Created_Date_Time__c = LAST_90_DAYS\n    \n    Returns:\n        A list of dictionaries where each dictionary represents a row from the query result.\n        Records where GCLID__c is None are filtered out.\n    \"\"\"\n    sf = get_salesforce_connection()\n    soql_query = (\n        \"SELECT Id, GCLID__c, Name, Original_Lead_Created_Date_Time__c, Admission_Date__c \"\n        \"FROM Opportunity \"\n        \"WHERE StageName IN ('Admitted', 'Alumni') \"\n        \"AND Original_Lead_Created_Date_Time__c = LAST_90_DAYS\"\n    )\n    # Use query_all to ensure all records are retrieved.\n    result = sf.query_all(soql_query)\n    records = result.get(\"records\", [])\n    # Remove attributes added by simple_salesforce for a clean response.\n    for record in records:\n        record.pop('attributes', None)\n    # Filter out records where GCLID__c is None\n    records = [record for record in records if record.get('GCLID__c') is not None]\n    return records\n"
        }
      ],
      "update_status": "to_update",
      "is_deployed": false
    },
    {
      "design": {
        "type": "logic",
        "name": "filter_unprocessed",
        "namespace": "app.pipeline",
        "dependencies": [
          "app.core.database.sql_adaptor",
          "app.models.UploadStatus"
        ],
        "purpose": "Filters out rows that have already been processed. It takes the raw sales data provided as input (from salesforce_query when called by the orchestration endpoint) and returns only those rows that haven't been processed yet. This module may also log a count of filtered rows.",
        "pypi_packages": [],
        "is_endpoint": false
      },
      "files": [
        {
          "path": "app/pipeline/filter_unprocessed.py",
          "content": "import os\nfrom typing import List, Dict, Any\nfrom app.core.database.sql_adaptor import SessionLocal\nfrom app.models.upload_status import UploadStatus\n\ndef filter_unprocessed(sales_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    \"\"\"\n    Filters out rows that have already been processed.\n    It checks each record from the raw sales data (provided by salesforce_query)\n    against the UploadStatus records in the database.\n    \n    A record is considered processed if its Salesforce ID is already present in the database.\n    \n    Args:\n        sales_data: List of dictionaries containing sales data, where each dictionary\n                    is expected to have an \"Id\" key representing the Salesforce Opportunity Id.\n    \n    Returns:\n        List of dictionaries that haven't been processed yet.\n        \n    Raises:\n        Any exceptions raised by database queries will propagate.\n    \"\"\"\n    # Open a new database session.\n    with SessionLocal() as session:\n        # Retrieve a set of all processed Salesforce IDs from the upload_status table.\n        processed_records = session.query(UploadStatus.salesforce_id).all()\n        processed_ids = {record[0] for record in processed_records}\n\n    # Filter sales data to include only rows which haven't been processed.\n    filtered_data = [row for row in sales_data if row.get(\"Id\") not in processed_ids]\n\n    # Log the count of filtered records.\n    filtered_count = len(sales_data) - len(filtered_data)\n    print(f\"[filter_unprocessed] Filtered out {filtered_count} records that were already processed.\")\n\n    return filtered_data\n"
        }
      ],
      "update_status": "up_to_date",
      "is_deployed": false
    },
    {
      "design": {
        "type": "logic",
        "name": "google_ads_upload",
        "namespace": "app.pipeline",
        "dependencies": [
          "app.core.google_auth.google_auth"
        ],
        "purpose": "Uploads conversions to Google Ads using the Google Ads API. This module takes filtered records (from filter_unprocessed) and performs conversion uploads through a REST API call. It requires an access token obtained from the google_auth module (app.core.google_auth.google_auth) for authentication. The module processes each filtered record by extracting the sales lead creation time from either 'Original_Lead_Created_Date_Time__c' or 'Admission_Date__c' and the GCLID from 'GCLID__c'.\n\nHardcoded values and details:\n\u2022 action_id: The conversion action identifier is hardcoded to 462477827.\n\u2022 API version: The Google Ads API version is hardcoded to \"v18\".\n\u2022 Date formatting: The input datetime is expected in ISO format with timezone information (formatted as \"%Y-%m-%dT%H:%M:%S.%f%z\"). It is then reformatted to \"%Y-%m-%d %H:%M:%S%z\" with an adjustment to the timezone format by inserting a colon.\n\u2022 Conversion details: Each conversion object is created with a static conversion value of 1 and a currency code of \"USD\".\n\u2022 Endpoint URL: The endpoint URL is built as \"https://googleads.googleapis.com/{api_version}/customers/{customer_id}:uploadClickConversions\" where api_version and customer_id are injected from the hardcoded API version ('v18') and environment variable GADS_CUSTOMER respectively.\n\u2022 Request headers: In addition to the Authorization header (Bearer token), headers include \"developer-token\" (from environment variable GADS_DEVELOPER_TOKEN) and \"login-customer-id\" (from environment variable GADS_LOGIN_CUSTOMER_ID).\n\u2022 Partial failure handling: If the response contains a 'partialFailureError', it extracts error details by mapping error indices to the individual conversion objects.\n\nThis module returns a list of result objects that include either error details or the result of a successful conversion upload.",
        "pypi_packages": [
          "google-ads==22.0.0"
        ],
        "is_endpoint": false
      },
      "files": [
        {
          "path": "app/pipeline/google_ads_upload.py",
          "content": "import os\nimport requests\nfrom datetime import datetime\nfrom typing import List, Dict, Any\n\nfrom app.core.google_auth.google_auth import get_access_token\n\n\ndef upload_conversions(filtered_records: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n    action_id = 462477827\n    customer_id = os.getenv(\"GADS_CUSTOMER\")\n    api_version = \"v18\"\n\n    access_token = get_access_token()\n\n    conversion_objects = []\n    for record in filtered_records:\n        lead_created_time = record.get(\n            \"Original_Lead_Created_Date_Time__c\"\n        ) or record.get(\"Admission_Date__c\")\n        gclid = record.get(\"GCLID__c\")\n\n        if not (gclid and lead_created_time):\n            continue\n\n        formatted_date = datetime.strptime(lead_created_time, \"%Y-%m-%dT%H:%M:%S.%f%z\")\n        formatted_date = formatted_date.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n        formatted_date = formatted_date[:-2] + \":\" + formatted_date[-2:]\n\n        conversion = {\n            \"conversionAction\": f\"customers/{customer_id}/conversionActions/{action_id}\",\n            \"gclid\": gclid,\n            \"conversionValue\": 1,\n            \"conversionDateTime\": formatted_date,\n            \"currencyCode\": \"USD\",\n        }\n        conversion_objects.append(conversion)\n\n    if not conversion_objects:\n        return []\n\n    url = f\"https://googleads.googleapis.com/{api_version}/customers/{customer_id}:uploadClickConversions\"\n    headers = {\n        \"Authorization\": f\"Bearer {access_token}\",\n        \"Content-Type\": \"application/json\",\n        \"developer-token\": os.getenv(\"GADS_DEVELOPER_TOKEN\"),\n        \"login-customer-id\": os.getenv(\"GADS_LOGIN_CUSTOMER_ID\"),\n    }\n    payload = {\n        \"conversions\": conversion_objects,\n        \"partialFailure\": True,\n    }\n\n    response = requests.post(url, headers=headers, json=payload)\n    if not response.ok:\n        print(response.text)\n        response.raise_for_status()\n\n    conversion_response = response.json()\n    results = []\n    if \"partialFailureError\" in conversion_response:\n        # Extract error details\n        error_details = conversion_response[\"partialFailureError\"][\"details\"][0][\n            \"errors\"\n        ]\n        error_by_index = {\n            error[\"location\"][\"fieldPathElements\"][0][\"index\"]: error\n            for error in error_details\n        }\n\n        # Map results with original GCLIDs\n        for idx, original_conversion in enumerate(conversion_objects):\n            result = {\"gclid\": original_conversion[\"gclid\"]}\n            if idx in error_by_index:\n                result[\"error\"] = error_by_index[idx]\n            else:\n                # Copy successful conversion data\n                result.update(conversion_response[\"results\"][idx])\n            results.append(result)\n    else:\n        results = conversion_response.get(\"results\", [])\n\n    return results\n"
        }
      ],
      "update_status": "up_to_date",
      "is_deployed": false
    },
    {
      "design": {
        "type": "logic",
        "name": "store_success",
        "namespace": "app.pipeline",
        "dependencies": [
          "app.core.database.sql_adaptor",
          "app.models.UploadStatus"
        ],
        "purpose": "Stores details of successful uploads into a database. This module logs each successful conversion upload along with metadata (e.g., timestamp, conversion id, status) for further audit and reporting. It utilizes SQLAlchemy sessions from the SQL adaptor for database interactions. The input data is provided from the google_ads_upload module when orchestrated by an endpoint.",
        "pypi_packages": [
          "sqlalchemy==2.0.36"
        ],
        "is_endpoint": false
      },
      "files": [
        {
          "path": "app/pipeline/store_success.py",
          "content": "import datetime\nfrom typing import List, Dict, Any\n\nfrom sqlalchemy.orm import Session\nfrom app.core.database.sql_adaptor import SessionLocal\nfrom app.models.upload_status import UploadStatus\n\ndef store_success_records(success_data: List[Dict[str, Any]]) -> List[UploadStatus]:\n    \"\"\"\n    Stores details of successful conversion uploads into the database.\n    \n    Each record in success_data is expected to be a dictionary with the following keys:\n      - 'salesforce_id': str - The Salesforce Opportunity Id.\n      - 'gclid': Optional[str] - The GCLID value from the Salesforce record.\n      - 'original_lead_created_datetime': datetime or ISO formatted str - When the lead was created.\n      - 'admission_date': datetime or ISO formatted str - The date of admission.\n      - 'status': str - The upload status, expected to be 'successful'.\n      - 'error_details': Optional[str] - Should be None or empty for successful uploads.\n      \n    Returns:\n        A list of UploadStatus instances that have been stored in the database.\n    \n    Raises:\n        Any exceptions raised during database operations.\n    \"\"\"\n    stored_records: List[UploadStatus] = []\n    \n    # Open a new database session.\n    with SessionLocal() as session:\n        # Iterate over each record from the input data.\n        for record in success_data:\n            # Parse datetime fields if they are provided as strings.\n            # If they are already datetime objects, they remain unchanged.\n            original_lead_created_datetime = record.get(\"original_lead_created_datetime\")\n            if isinstance(original_lead_created_datetime, str):\n                original_lead_created_datetime = datetime.datetime.fromisoformat(original_lead_created_datetime)\n            \n            admission_date = record.get(\"admission_date\")\n            if isinstance(admission_date, str):\n                admission_date = datetime.datetime.fromisoformat(admission_date)\n            \n            upload_status = UploadStatus(\n                salesforce_id=record[\"salesforce_id\"],\n                gclid=record.get(\"gclid\"),\n                original_lead_created_datetime=original_lead_created_datetime,\n                admission_date=admission_date,\n                status=record[\"status\"],\n                error_details=record.get(\"error_details\")\n            )\n            \n            session.add(upload_status)\n            stored_records.append(upload_status)\n        \n        session.commit()\n        # Refresh all instances to reflect data from the database (e.g., autogenerated id, timestamp)\n        for rec in stored_records:\n            session.refresh(rec)\n    \n    return stored_records\n"
        }
      ],
      "update_status": "up_to_date",
      "is_deployed": false
    },
    {
      "design": {
        "type": "logic",
        "name": "google_sheets_report",
        "namespace": "app.pipeline",
        "dependencies": [
          "app.core.google_auth.google_auth"
        ],
        "purpose": "Creates a Google Sheets report that contains both successful and failed upload records. This module requires an access token provided by the google_auth module (app.core.google_auth.google_auth) to authenticate API requests to the Google Sheets API. It generates a Google Sheets document, populates it with data provided by previous steps (google_ads_upload and store_success), and applies formatting for better readability.",
        "pypi_packages": [
          "gspread==5.10.0",
          "google-auth==2.16.0"
        ],
        "is_endpoint": false
      },
      "files": [
        {
          "path": "app/pipeline/google_sheets_report.py",
          "content": "import os\nfrom typing import List, Dict, Any\n\nimport gspread\nfrom google.oauth2.credentials import Credentials\n\nfrom app.core.google_auth.google_auth import get_google_oauth_credentials\n\ndef create_spreadsheet_report(success_records: List[Dict[str, Any]], \n                                failed_records: List[Dict[str, Any]]) -> str:\n    \"\"\"\n    Creates a Google Sheets report containing both successful and failed upload records.\n    Uses credentials from app.core.google_auth.google_auth to authenticate with Google Sheets API.\n    \n    Args:\n        success_records: List of dictionaries representing successful uploads.\n        failed_records: List of dictionaries representing failed uploads.\n    \n    Returns:\n        URL of the created Google Sheets document.\n    \n    Raises:\n        Any exceptions raised by gspread or google-auth.\n    \"\"\"\n    # Obtain OAuth2 credentials required by gspread\n    creds: Credentials = get_google_oauth_credentials()\n    \n    # Authorize gspread client using the credentials\n    client: gspread.Client = gspread.authorize(creds)\n    \n    # Define spreadsheet title from environment variable or default value\n    sheet_title: str = os.getenv(\"GOOGLE_SHEETS_REPORT_TITLE\", \"Uploads Report\")\n    \n    # Create new spreadsheet\n    spreadsheet = client.create(sheet_title)\n    \n    # Share the spreadsheet with the service account email if provided\n    service_account_email: str = os.getenv(\"SERVICE_ACCOUNT_EMAIL\", \"\")\n    if service_account_email:\n        spreadsheet.share(service_account_email, perm_type='user', role='writer')\n\n    # Prepare header rows for both worksheets\n    success_headers = [\"Salesforce ID\", \"GCLID\", \"Original Lead Created\", \"Admission Date\", \"Status\", \"Error Details\"]\n    failed_headers = [\"Salesforce ID\", \"Error Details\"]\n    \n    # Create a worksheet for successful uploads and populate with header and data rows\n    try:\n        worksheet_success = spreadsheet.worksheet(\"Successful Uploads\")\n    except gspread.WorksheetNotFound:\n        worksheet_success = spreadsheet.add_worksheet(title=\"Successful Uploads\", rows=\"100\", cols=\"20\")\n    worksheet_success.clear()\n    worksheet_success.append_row(success_headers)\n    for record in success_records:\n        row = [\n            record.get(\"salesforce_id\", \"\"),\n            record.get(\"gclid\", \"\"),\n            record.get(\"original_lead_created_datetime\", \"\"),\n            record.get(\"admission_date\", \"\"),\n            record.get(\"status\", \"\"),\n            record.get(\"error_details\", \"\")\n        ]\n        worksheet_success.append_row(row)\n    \n    # Create a worksheet for failed uploads and populate with header and data rows\n    try:\n        worksheet_failed = spreadsheet.worksheet(\"Failed Uploads\")\n    except gspread.WorksheetNotFound:\n        worksheet_failed = spreadsheet.add_worksheet(title=\"Failed Uploads\", rows=\"100\", cols=\"10\")\n    worksheet_failed.clear()\n    worksheet_failed.append_row(failed_headers)\n    for record in failed_records:\n        row = [\n            record.get(\"salesforce_id\", \"\"),\n            record.get(\"error\", \"\")\n        ]\n        worksheet_failed.append_row(row)\n    \n    # Apply basic formatting (e.g., set column widths) if needed.\n    # This example sets format by auto-resizing columns for both sheets.\n    worksheet_success.resize(rows=1+len(success_records))\n    worksheet_failed.resize(rows=1+len(failed_records))\n    \n    # Return the URL of the created spreadsheet\n    return spreadsheet.url\n\ndef generate_google_sheets_report(success_records: List[Dict[str, Any]], \n                                  failed_records: List[Dict[str, Any]]) -> Dict[str, Any]:\n    \"\"\"\n    Generates a Google Sheets report containing successful and failed upload records.\n    This function is designed to be called by the pipeline orchestration service.\n    \n    Args:\n        success_records: List of dicts with details of successful uploads.\n        failed_records: List of dicts with details of failed uploads.\n    \n    Returns:\n        A dictionary containing:\n            - 'spreadsheet_url': URL to the Google Sheets report.\n            - 'success_count': Number of successful records.\n            - 'failed_count': Number of failed records.\n    \n    Raises:\n        Any exceptions raised during report creation.\n    \"\"\"\n    spreadsheet_url: str = create_spreadsheet_report(success_records, failed_records)\n    \n    report_summary: Dict[str, Any] = {\n        \"spreadsheet_url\": spreadsheet_url,\n        \"success_count\": len(success_records),\n        \"failed_count\": len(failed_records)\n    }\n    return report_summary\n"
        }
      ],
      "update_status": "to_remove",
      "is_deployed": false
    },
    {
      "design": {
        "type": "logic",
        "name": "gmail_notification",
        "namespace": "app.pipeline",
        "dependencies": [
          "app.core.google_auth.google_auth"
        ],
        "purpose": "Sends an email using Gmail containing the report link from the Google Sheets document. This module requires an access token provided by the google_auth module (app.core.google_auth.google_auth) to authenticate API calls to the Gmail API. It composes an email summarizing the successful and failed upload counts, attaches the report link, and sends the email via the Gmail API. It handles OAuth authentication and ensures appropriate error handling for email delivery.",
        "pypi_packages": [
          "google-api-python-client==2.70.0",
          "google-auth==2.16.0"
        ],
        "is_endpoint": false
      },
      "files": [
        {
          "path": "app/pipeline/gmail_notification.py",
          "content": "import os\nimport base64\nfrom email.mime.text import MIMEText\nfrom typing import Dict\n\nfrom googleapiclient.discovery import build\nfrom google.auth.credentials import Credentials\nfrom app.core.google_auth.google_auth import get_google_oauth_credentials\n\n\ndef create_email_message(sender: str, to: str, subject: str, body: str) -> Dict[str, str]:\n    \"\"\"\n    Creates a raw email message for Gmail API.\n\n    Args:\n        sender: Email address of the sender.\n        to: Email address of the receiver.\n        subject: Subject of the email.\n        body: Body text of the email.\n\n    Returns:\n        A dictionary containing the raw email message ready for the Gmail API.\n    \"\"\"\n    message = MIMEText(body)\n    message[\"to\"] = to\n    message[\"from\"] = sender\n    message[\"subject\"] = subject\n    # Gmail API requires base64url encoded string.\n    raw_message = base64.urlsafe_b64encode(message.as_bytes()).decode()\n    return {\"raw\": raw_message}\n\n\ndef send_email_report(success_count: int, failed_count: int, report_url: str) -> Dict[str, str]:\n    \"\"\"\n    Sends an email using the Gmail API containing the report link of the Google Sheets document.\n\n    The email summarizes the successful and failed upload counts and includes the URL of the report.\n    Relies on Google OAuth credentials from the google_auth module.\n\n    Environment Variables:\n        GMAIL_SENDER: Sender's email address.\n        GMAIL_RECIPIENT: Recipient's email address.\n        GMAIL_EMAIL_SUBJECT: Subject line for the email (optional, default provided).\n\n    Args:\n        success_count: Number of successful uploads.\n        failed_count: Number of failed uploads.\n        report_url: URL string of the Google Sheets report.\n\n    Returns:\n        A dictionary with keys 'status' and 'message' indicating the result of the email delivery.\n    \n    Raises:\n        googleapiclient.errors.HttpError: When the Gmail API call fails.\n    \"\"\"\n    gmail_sender = os.getenv(\"GMAIL_SENDER\")\n    gmail_recipient = os.getenv(\"GMAIL_RECIPIENT\")\n    email_subject = os.getenv(\"GMAIL_EMAIL_SUBJECT\", \"Upload Report Notification\")\n\n    if not gmail_sender or not gmail_recipient:\n        raise EnvironmentError(\"GMAIL_SENDER and GMAIL_RECIPIENT environment variables must be set.\")\n\n    # Prepare the email body\n    email_body = (\n        f\"Hello,\\n\\n\"\n        f\"Please find the upload report details below:\\n\"\n        f\"Successful Uploads: {success_count}\\n\"\n        f\"Failed Uploads: {failed_count}\\n\"\n        f\"Report URL: {report_url}\\n\\n\"\n        f\"Best regards,\\n\"\n        f\"Your Pipeline Service\"\n    )\n\n    message = create_email_message(gmail_sender, gmail_recipient, email_subject, email_body)\n\n    # Get credentials from our google_auth module\n    creds: Credentials = get_google_oauth_credentials()\n\n    # Build the Gmail service\n    service = build(\"gmail\", \"v1\", credentials=creds)\n\n    # Send the message using the Gmail API\n    sent_message = service.users().messages().send(userId=\"me\", body=message).execute()\n    return {\"status\": \"success\", \"message\": f\"Email sent successfully, id: {sent_message.get('id')}\"}\n"
        }
      ],
      "update_status": "to_remove",
      "is_deployed": false
    },
    {
      "design": {
        "type": "logic",
        "name": "pipeline_endpoint",
        "namespace": "app.pipeline",
        "dependencies": [
          "app.pipeline.salesforce_query",
          "app.pipeline.filter_unprocessed",
          "app.pipeline.google_ads_upload",
          "app.pipeline.store_success"
        ],
        "purpose": "Defines a FastAPI endpoint that orchestrates the entire data pipeline by sequentially invoking several modules and aggregating their outputs. When this endpoint is accessed, it performs the following hardcoded steps:\n\n1. Calls salesforce_query to fetch data from Salesforce.\n2. Calls filter_unprocessed to filter out already processed records.\n3. Calls google_ads_upload to upload conversions to Google Ads.\n4. Uses a partitioning function to separate successful and failed upload results. The partitioning iterates over the upload results, matching each result with its corresponding record by GCLID. Success is determined by the presence of a success indicator in the result.\n5. Calls store_success to save successful conversions in the database.\n\nLogging is added between each step for debugging. Returns a list of the successful uploads.",
        "pypi_packages": [
          "fastapi==0.115.6"
        ],
        "is_endpoint": true
      },
      "files": [],
      "update_status": "to_update",
      "is_deployed": false
    },
    {
      "design": {
        "type": "datamodel",
        "name": "UploadStatus",
        "namespace": "app.models",
        "dependencies": [
          "app.core.database.sql_adaptor"
        ],
        "fields": [
          {
            "name": "id",
            "purpose": "Primary key for the upload record entry."
          },
          {
            "name": "salesforce_id",
            "purpose": "The Salesforce Opportunity Id from the query result."
          },
          {
            "name": "gclid",
            "purpose": "The GCLID value (GCLID__c) from the Salesforce record, used for conversion tracking."
          },
          {
            "name": "original_lead_created_datetime",
            "purpose": "The Original_Lead_Created_Date_Time__c field from the Salesforce record capturing when the lead was created."
          },
          {
            "name": "admission_date",
            "purpose": "The Admission_Date__c field from the Salesforce record indicating the date of admission."
          },
          {
            "name": "status",
            "purpose": "Upload status indicator, e.g., 'successful' or 'failed'."
          },
          {
            "name": "timestamp",
            "purpose": "The date and time when this upload status was recorded."
          },
          {
            "name": "error_details",
            "purpose": "Contains error details if the upload failed, otherwise null."
          }
        ]
      },
      "files": [
        {
          "path": "app/models/upload_status.py",
          "content": "from sqlalchemy import Column, Integer, String, DateTime, Text\nfrom app.core.database.sql_adaptor import Base\nimport datetime\n\nclass UploadStatus(Base):\n    __tablename__ = \"upload_status\"\n\n    id = Column(Integer, primary_key=True, index=True)\n    salesforce_id = Column(String, nullable=False, comment=\"Primary key for the upload record entry: Salesforce Opportunity Id from the query result.\")\n    gclid = Column(String, nullable=True, comment=\"The GCLID value (GCLID__c) from the Salesforce record, used for conversion tracking.\")\n    original_lead_created_datetime = Column(DateTime, nullable=False, comment=\"The Original_Lead_Created_Date_Time__c field from the Salesforce record capturing when the lead was created.\")\n    admission_date = Column(DateTime, nullable=False, comment=\"The Admission_Date__c field from the Salesforce record indicating the date of admission.\")\n    status = Column(String, nullable=False, comment=\"Upload status indicator, e.g., 'successful' or 'failed'.\")\n    timestamp = Column(DateTime, default=datetime.datetime.utcnow, nullable=False, comment=\"The date and time when this upload status was recorded.\")\n    error_details = Column(Text, nullable=True, comment=\"Contains error details if the upload failed, otherwise null.\")\n    \n    def __repr__(self):\n        return (f\"<UploadStatus(id={self.id}, salesforce_id='{self.salesforce_id}', \"\n                f\"gclid='{self.gclid}', original_lead_created_datetime='{self.original_lead_created_datetime}', \"\n                f\"admission_date='{self.admission_date}', status='{self.status}', \"\n                f\"timestamp='{self.timestamp}', error_details='{self.error_details}')>\")\n"
        },
        {
          "path": "app/models/upload_status_schema.py",
          "content": "from datetime import datetime\nfrom typing import Optional\nfrom pydantic import BaseModel\n\nclass UploadStatusBase(BaseModel):\n    salesforce_id: str\n    gclid: Optional[str] = None\n    original_lead_created_datetime: datetime\n    admission_date: datetime\n    status: str\n    error_details: Optional[str] = None\n\nclass UploadStatusCreate(UploadStatusBase):\n    pass\n\nclass UploadStatusRead(UploadStatusBase):\n    id: int\n    timestamp: datetime\n\n    class Config:\n        orm_mode = True\n"
        }
      ],
      "update_status": "up_to_date",
      "is_deployed": false
    },
    {
      "design": {
        "type": "logic",
        "name": "google_auth",
        "namespace": "app.core.google_auth",
        "dependencies": [],
        "purpose": "Handles Google OAuth authentication using credentials from environment variables. It uses OAUTH_CLIENT_ID, OAUTH_CLIENT_SECRET, and OAUTH_REFRESH_TOKEN to obtain and refresh access tokens. This module provides the access token required for interacting with the Google Ads API, Google Sheets API, and Gmail API.",
        "pypi_packages": [
          "google-auth==2.16.0"
        ],
        "is_endpoint": false
      },
      "files": [
        {
          "path": "app/core/google_auth/google_auth.py",
          "content": "import os\nfrom typing import Dict\nfrom google.oauth2.credentials import Credentials\nfrom google.auth.transport.requests import Request\n\n# OAuth token endpoint for Google\nGOOGLE_TOKEN_URI = \"https://oauth2.googleapis.com/token\"\n\ndef get_google_oauth_credentials() -> Credentials:\n    \"\"\"\n    Obtains Google OAuth credentials by using environment variables:\n      - OAUTH_CLIENT_ID\n      - OAUTH_CLIENT_SECRET\n      - OAUTH_REFRESH_TOKEN\n\n    Creates a Credentials object with no initial access token and refreshes it immediately,\n    returning valid credentials with an access token.\n    \n    Returns:\n        A google.oauth2.credentials.Credentials instance with a valid access token.\n    \n    Raises:\n        EnvironmentError: If one or more required environment variables are not set.\n        google.auth.exceptions.RefreshError: If the token refresh operation fails.\n    \"\"\"\n    client_id = os.getenv(\"OAUTH_CLIENT_ID\")\n    client_secret = os.getenv(\"OAUTH_CLIENT_SECRET\")\n    refresh_token = os.getenv(\"OAUTH_REFRESH_TOKEN\")\n\n    if not (client_id and client_secret and refresh_token):\n        raise EnvironmentError(\"One or more required environment variables (OAUTH_CLIENT_ID, OAUTH_CLIENT_SECRET, OAUTH_REFRESH_TOKEN) are not set.\")\n\n    # Construct a credentials object without an access token\n    creds = Credentials(\n        token=None,\n        refresh_token=refresh_token,\n        token_uri=GOOGLE_TOKEN_URI,\n        client_id=client_id,\n        client_secret=client_secret\n    )\n    \n    # Refresh the access token using a Request\n    creds.refresh(Request())\n    return creds\n\ndef get_access_token() -> str:\n    \"\"\"\n    Obtains a valid Google OAuth access token.\n    \n    Returns:\n        The access token as a string.\n    \n    Raises:\n        EnvironmentError: If required environment variables are missing.\n        google.auth.exceptions.RefreshError: If token refresh fails.\n    \"\"\"\n    creds = get_google_oauth_credentials()\n    return creds.token\n\ndef get_google_auth_headers() -> Dict[str, str]:\n    \"\"\"\n    Returns HTTP headers for authenticating against Google APIs.\n    The header contains the Bearer token.\n    \n    Returns:\n        A dictionary with the Authorization header.\n    \"\"\"\n    access_token = get_access_token()\n    return {\"Authorization\": f\"Bearer {access_token}\"}\n"
        }
      ],
      "update_status": "up_to_date",
      "is_deployed": false
    }
  ],
  "github": "https://github.com/Modular-Asembly/lgaleana_jts2"
}